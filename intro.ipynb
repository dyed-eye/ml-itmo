{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy matplotlib autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression\n",
    "\n",
    "**Goal**: teach computer to predict a numerical value given some input\n",
    "\n",
    "**Model**. Consider approach based on regression analyis.\n",
    "[Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis) is a statistical technique to estimate dependence between variables. \n",
    "Suppose we have samples $x_n$ of a random value $X$ and the corresponding samples $y_n$ of a random value $Y$.\n",
    "We assume $Y$ is a function $f$ of $X$ plus an error term independent of $X$:\n",
    "\n",
    "$$\n",
    "Y=f(X;\\theta)+W.\n",
    "$$\n",
    "\n",
    "The function $f$ defines possible relationship between the variables and the parameters $\\theta$ controls exact form of the dependence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1.** Implement in Python function to sample the function $y=\\sin x$ with addition of normally distributed noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import autograd as ag\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "S = 100 # sample size\n",
    "X = np.random.rand(S)*2 # inicialise X array\n",
    "# print(f'min X = {np.min(X)}, max X = {np.max(X)}') \n",
    "\n",
    "W = np.random.randn(S)*0.01 # initialize noise\n",
    "Y = np.sin(X) + W # create Y samples as sin function with noise\n",
    "plt.plot(X,Y, '.') # ploting samples\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('$Y=f(X)+W$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approaches**. In fact as far as we need to find best function that approximate input, we will solve an optimization problem. Thus we need to define something to optimize. Mainly two functions are used: **loss** and **likelihood** functions.\n",
    "\n",
    "**Minimum loss function principle**.\n",
    "Function $f$ is a solution controlled by the parameter $\\theta$. The optimal $\\theta$ provides the best fitting function. To find optimal value we need to solve optimization problem or find minimum of some error function. Define **Loss function**:\n",
    "\n",
    "$$\n",
    "R[x,y] = ||y - f(x,\\theta)||^2\n",
    "$$\n",
    "\n",
    "The solution of optimisation problem is $\\theta_{opt} = \\mathrm{argmin}_\\theta(L)$\n",
    "\n",
    "In practice, the estimate of $\\theta$ is obtained by the [least squares method](https://en.wikipedia.org/wiki/Least_squares)\n",
    "from minimization of the mean square error:\n",
    "\n",
    "$$\n",
    "R[\\theta] = \\sum_n (y_n-f(x_n,\\theta))^2.\n",
    "$$\n",
    "\n",
    "The minimum of $R$ can be found from the necessary condition of optimality, which is sufficient in the case of quadratic functional:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial R}{\\partial\\theta_k} = 2\\sum_n(y_n-f(x_n,\\theta_k))\\frac{\\partial f}{\\partial \\theta_k}(x_n,\\theta) = 0\\forall k.\n",
    "$$\n",
    "\n",
    "**Example 2.** Implement a function that solve linear least squares method for decomposition of $y$ over polynomials of degree $D$\n",
    "on the interval $[0,1]$\n",
    "using Moore-Penrose inverse:\n",
    "\n",
    "$$\n",
    "f(x,\\theta)=\\sum_{k=0}^D \\theta_k x^k.\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.09983341666666667 0.09983341664682815\n"
     ]
    }
   ],
   "source": [
    "def f(x, theta):\n",
    "  xn = np.power(x, np.arange(len(theta)))\n",
    "  return np.dot(xn, theta)\n",
    "\n",
    "# theta =        1, x, x^2,   x^3, x^4,       x^5\n",
    "theta_optimal = np.array([0, 1,   0,-1/2/3,   0, 1/2/3/4/5])\n",
    "\n",
    "x0 = 0.1 \n",
    "print(x0, f(x0,theta_optimal), np.sin(x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArcElEQVR4nO3de3yb9Xn38c8l2XICYaUYOg5JSBrCCsRQ0jRFpIDScIYuLaZHwBRoHdpka3g2GNlWmtcDDynQ0tByilsO9dbCsweXDGhaUtyIQCMGCTDMoWk5pOCFreABA5JYtnQ9f9yyIjt2LB90sr/v10sv+db9063L8m1d+h3u38/cHREREYBQqQMQEZHyoaQgIiJZSgoiIpKlpCAiIllKCiIiklVV6gBGYt999/Vp06aVOgwRkYqyadOmN919v/72VXRSmDZtGhs3bix1GCIiFcXM/jjQPjUfiYhIlpKCiIhkKSmIiEhWRfcp9Kerq4v29nZ27NhR6lAqxoQJE5g8eTLV1dWlDkVESmzMJYX29nb22msvpk2bhpmVOpyy5+50dHTQ3t7O9OnTSx2OiJTYmGs+2rFjB7W1tUoIeTIzamtrVbMSEWAMJgVACWGI9H5JKSUSsGJFcC+lN+aaj0SkciQSsGABJJMQiUBrK0SjpY5qfBuTNYXxJBaL6QI+qVjxeJAQUqngPh4vdUSimoKIlEwsFtQQemoKsVj/5RKJIGHEYkFNIncbeu/L5/kysKIkBTO7HTgT+JO7z+pnvwE3AKcD24CvuPuTxYgNRv+Eef/99/n85z9Pe3s7qVSKb33rW2zevJn777+f7du3c+yxx7Jq1SrMjFgsxtFHH82mTZt44403aG5uZsWKFbS1tfGFL3yBq666ii1btnDqqafyiU98gqeeeopDDz2U5uZm9thjj16vu3btWr797W/T2dnJjBkzuOOOO5g0aRKXX3459913H1VVVZx88sl897vfHfkvKTIKotGgyai5eeAyfZuYVq6EpUuDbTNwD241Nf03P6mJamiKVVO4E7gRGOhPfxowM3P7BHBL5r7gCnHC/OpXv+LAAw/kF7/4BQDvvPMOJ510EldccQUA5513Hg888ACf/vSnAYhEIqxfv54bbriBhQsXsmnTJvbZZx9mzJjBJZdcAsDmzZu57bbbmDdvHhdeeCE333wzf/u3f5t9zTfffJOrrrqKhx56iD333JNrrrmG66+/niVLlnDvvffyu9/9DjPj7bffHtkvJ5KHoXzRmtSW4OgfNXNo6nn+/NZXeXniRCYdN5sP8QbU1xPvaGR2Z4Lj0nEe6YzR0hLNbv8ZbzOfODuYwO+2H84fmhuIQq8Xj8fJlv/vHbXULH0KZgMNDcoO/XH3otyAacCzA+xbBXwpZ3szcMBgx/zYxz7mfT3//PO7PLY7V1/tHg4H3zXC4WB7pDZv3uzTpk3zyy67zNevX+/u7vfcc4/PnTvXZ82a5QceeKCvWLHC3d1POOEEf/TRR93dvbW11U888cTscY477jh/6qmn/JVXXvEpU6ZkH29tbfWFCxdmn//EE0/4/fff77W1tX7UUUf5UUcd5YcddphfeOGF3tXV5UceeaRfeOGF3tLS4p2dnf3GPNT3TWQgGza4T5wY/D9NnBhs91vo6qvdV63yrnCNp6Hfm4O/ds5l/j4TvYuwv89Ej5+zKrNtu5RPhavda2p6vfgzqzZkyod6HdfDYfdVq4J4zjnHfZ99gvtxANjoA3yulkufwkHAaznb7ZnHXi/0C+fbpjkUhx56KJs2bWLNmjUsW7aMk08+mZtuuomNGzcyZcoUli9f3uu6gJqaGgBCoVD2557t7u5uYNdho3233Z2TTjqJu+66a5d4Hn/8cVpbW7n77ru58cYb+c1vfjPyX1JkALnfzB/pjBGPR3t/Ic+tnpsRTqXoOyjaIfvY5H/7OR5KYukU4VCSE95oyWx7r3IAlurC04a5451JLB6nDjLl073KeyqFf2MxoZYWfO3a4MGf/jTYv3gxr1/bzOtbofqiBuoax0+NolySQn8D5b3fgmaNQCPA1KlTR/zCPW2ao9mnsHXrVvbZZx/OPfdcJk2axJ133gnAvvvuy3vvvcc999zD2WefPaRjvvrqqyQSCaLRKHfddRef/OQne+0/5phjWLx4MS+++CKHHHII27Zto729nQMPPJBt27Zx+umnc8wxx3DIIYeM/BcU6dFPO9GZtQm+mV5AhCTJdISXaluBnH+s3CFHoRAWDuGp1C6H7vkAsLPOwn74Q0gmsUgE6uuxRx7Bt+8AvNcHRSpcTXcqRJhuutIRXqqNUVcHVhPBOzshkxgg+NBJpdKk1j1CVWbbge5778P+5f+xf1eS/YHOx++gjXVBYhgHPdblkhTagSk525OBrf0VdPcmoAlgzpw5/SaOoYpGR/fv29bWxqWXXkooFKK6uppbbrmF1atXU1dXx7Rp0/j4xz8+5GMedthh/OQnP2HRokXMnDmTr3/9673277ffftx555186UtforOzE4CrrrqKvfbai4ULF7Jjxw7cne9///uj8jvK+NLvZ2EiQWr+AiyZxCMRwuuCDrm6jnivb/Z1HXF6JYVYjFRVBNJJqI4Q/sFK7KmneOmB57H2V9nGRJ4k6FNooZ6Pz2iksfUzvQOoq8Picdqfe5vkr+O89qcJvGCH81MaSBsc73EeCcU4oyNKXRRobeXh5XF+traWU/kln+Z+DCdJDU9POI5o19psstg68cNM3vZM9ptqNUk6WuJQx/josR6oXWm0b+y+T+EM4JcEyfoY4PF8jjkafQqV4JVXXvEjjjiioK8xFt83Gbqepv7cfoANG9xPiGzwv7er/YTIhuy+LRdf7V0EHXJJwr7l4qt3PmE3nQoDHa9n3yGH9IwnCm4nn7z7mHP7BUMh9+rq/l96wwb3SCQodwwb/HKu9uOqNvhHP+r+E87xP7GP/4RzvGHmBt9OJNv/sJ0af2bVhsJ0QJYIpe5TMLO7gBiwr5m1A98GqjNJ6VZgDcFw1BcJhqReUIy4RGSngUbi/aE5wZpkpjkoGeGe5lai0SgPE+NsIjhJuojwMDEaYNA22XgcHk1FedijhFPBdk+RaBQuvRQWLdpZvr5+93H37RdcuRI6OnZ96Wg0eK3mZvjP/4zy9v5RrmkIhreezz9ny03eDieG4pyTDgZLvvuZBi5rjEKC4XVAVlqT00DZohJu46WmUAx63+Tqq93nhYJv0PNCG7JfhAeqEezuG//u5DM6adWqoIbQMzgon2P2reHka9Wq3jWTyy4bOL5nVm3wdSdfHdQc8g0s92DnnON+0EHuxx8/vGBHCbupKZT8g30kNyWF0aP3TXYO3QyGfmY/+DZs8O6aid5tYe+u6f0pOdwP45F8iBdC3yQ0UDPaoENt+8ptcuo73LaqqmRvwO6SQrl0NItIiQ3YQRyNBp3I/TSBDHeQxmgP7hipxsbg1qO/+Pqbp2nQ3yGnU908jeE7h8R2d2O5B2lqgpaWoL0sN5giU1IQGS8Ga9uOxbCayM6hn7lt5uX2KV4Cw7mmKUGUZd7KPOJ8hOc4l59mRzmlrYpwz0GamvCejpS1a4PEUaLEoKQgMh7kM59LIS7aGUOG8/bkdqqHQrA1fRBf4qe8zAy+XfUdvkOUKPDWbS3szc5rJd66rYUP1tXtnBSqiFNyaOrsMrBy5Uq2bduW3T799NNHZY6ieDzOmWeeOeLjyBgQj+OdQduHd+5mjupoFJYtU0IYwFDfnp7aRTgcTNi35vhrOJh25vMwv01Hs3+GxIHBEKueWsQLEz4K8+fDrbcGt1isaKsQKSmUgb5JYc2aNey9996lC0gq026WMGurjbE9HaGLMNvTEdpqY8WPbxzqqV1ceWUwVPaxx3buq6ra2QTVflojjaziQU6mkVVU7bN3UKvr0dVVtMUmlBSgIOsBXn/99cyaNYtZs2axcuVKtmzZwkc+8hHOP/98jjzySM4++2y2bdvGD37wA7Zu3cr8+fOZP38+ANOmTePNN9/MPuerX/0qs2bN4pxzzuGhhx5i3rx5zJw5k8cffxwI5jY69thjOfroozn22GPZvHnzqP0eUn76PV0zVxen/+FbpOYv2OVcfqAjysmhVq7gSk4OtfJAh2oCxdJTu+jogMxUZpjBBRfsrHF0dMDtoUZO40FuDzXyu/1jpKsiOEHtIVVV3bsTo5BrmA40LKkSbqMyJHVY48x2b+PGjT5r1ix/7733/N133/XDDz/cn3zySQeyM6JecMEFft1117m7+8EHH+xvvPFG9vk926+88oqHw2F/5plnPJVK+ezZs/2CCy7wdDrtq1evzs6U+s4773hXV5e7u//617/2s846y93d161b52eccUZeMWtIamUY6HQd8OriQZ4nxdP3eojcazD6/n1WrXI/vnqD38zFfjMX+3FVOdeBjMIfk90MSVVNoQDrAT766KN89rOfZc8992TSpEmcddZZPPLII0yZMoV58+YBcO655/Loo48Oeqzp06dTV1dHKBTiiCOOYMGCBZgZdXV1bNmyBQjWa/jc5z7HrFmzuOSSS3juuedG/DtIeeqZgfTS1Apmdyayp+vDxEgSNA/1XF2cK7cZY6xO2VPuOjoglPnEDYWC7R59/z4dHfBId5RvcAvf4BYeTe3sfyj0GqYafVSAubODRLyrwaa/7k/fqbRzp9numVb7W9/6FvPnz+fee+9ly5YtxEZj/m8pSwPNQDqzIcrpt7cyryvOb6tjrGjY9VNfo0pLKxYLOpsH+qjp+/eJRCAztyXVua1HhZjvP4dqCgX4CnX88cezevVqtm3bxvvvv8+9997Lcccdl53+Gug1/fVee+3Fu+++O+zXe+eddzjooIMAstN0S2UbqMm4riPOxFCSKlJMzF5gFpy2K+JRJv2fZazou36BlIWhfNREo7BuHVx8cXDrdaFcgat9qinAqH+Fmj17Nl/5yleYO3cuAF/96lf54Ac/OOD0142NjZx22mkccMABrFu3bsivd9lll3H++edz/fXX86lPfWrUfg8pjd1eUrCbC8xUEyh/Q/kb7bZsAf/YNlBTRyWYM2eOb9y4sddjL7zwAocddliJIhrYli1bOPPMM3n22WdLHUq/yvV9G49WrIBf/GNm5bJQjDOuirJsWU6BSpt1U8qOmW1y9zn97VNNQaTMDLpymaoE414hvxcoKRTJtGnTyraWIOVl0JXLZFzLZ8aSkRiTHc2V3CRWCnq/ykxPv0E4HNxrNJnkKPCI1LFXU5gwYQIdHR3U1tbmNeRzvHN3Ojo6mDBhQqlDkR6amE52o8AjUsdeR3NXVxft7e3s2LGjRFFVngkTJjB58mSqq6tLHYqI5GGkfQrjqqO5urqa6dOnlzoMEZGCKeRYgzHZpyAiIsOjpCAiIllKCiIikqWkICIiWUoKIqOorSlB/JQVtDUVZ+lEkdE25kYfiZRKW1OCGYsWcBhJkmsjtNFKXaOuMZDKopqCyCjpaIkTIZjWupokHS3xUockMmRKCiKjpLa+9+pntfWxUockMmRqPhIZJXWNUdpopaMlTm19TE1HUpGUFERGUV1jFJQMpIKp+UhkEAMtjSkyFhWtpmBmpwI3AGHgx+7+nT77PwD8MzA1E9d33f2OYsUn0p9Cz10vUm6KUlMwszBwE3AacDjwJTM7vE+xxcDz7n4UEAO+Z2aRYsQnMpB4HM7b0cQDqVM4b0fTqM9dL1JuilVTmAu86O4vA5jZ3cBC4PmcMg7sZcEiCJOA/wa6ixSfSC89UxMf+1wTl/siAE7xtbz8NkBjCSMTKaxiJYWDgNdyttuBT/QpcyNwH7AV2Av4grunixOeyE6JBCyLJZjXFWeirwbACL617BNvQUlBxrJiJYX+lkDru7rPKcDTwKeAGcCvzewRd/+fXgcyayTzXzl16tTRj1TGvT80J1iTXECEJOlMC2vPyZo4sJ7TSxeaSMEVa/RROzAlZ3syQY0g1wXAzz3wIvAK8JG+B3L3Jnef4+5z9ttvv4IFLOPXCey8MtlI86/2GdZyMourVvHBy1RLkLGtWEnhCWCmmU3PdB5/kaCpKNerwAIAM/tz4C+Al4sUn0jWwQ0xrCZCysKEaiLMuPUynrz6Qc5b36iRRzLmFaX5yN27zWwJ8CDBkNTb3f05M7s4s/9W4ErgTjNrI2hu+jt3f7MY8cn4kdfattEo4XWt2YJ10Sh1xQtRpKTMvW/TfuWYM2eOb9y4sdRhSIXQNQciATPb5O5z+tunK5pl3IjHg4SQSgX3uuZAZFdKCjJuxGJBDSEchq/RxF/efAov/V1TqcMSKSuaEE/GhZ6+hJ/9VYKD/++1fPSPq4Mxcdeu5SVgxjUaVSQCSgoyRuV2KEPQlzC7M8E30wuYwHZg5wVpO37WAkoKIoCSgoxBfTuUzz8/SAj/mF5OhE5CBMmgZ4jFUx+u54gSxitSTpQUZMzp26E8/T8TfC+9gAidhEmTthBdHuIpZtMcvojzvqNagkgPJQUZc3o6lHtqCl/YP87EUBJLp/FQiNCJJ/L7+uWs64hyXkzDUkVyKSnImBONwr+tTGSXxTy4LgY/CbKERSKwfLkuSBMZgJKCjD2JBHVLM50Kj2SuUmttzeNSZhFRUpCxp7+r1JYtUzIQyYOSgowdPeNQa2t7dyr0jEsVkUEpKcjY0Hcc6sqV0NGh5iKRIVJSkMrW1AQtLbDHHr2bjDo6giYjERkSJQWpXE1N+KJF2c10uAosDFURwmoyEhkWTYgnFeut21qAnWu9bkzP5gquZIG3kkBNRiLDoaQgFesXE+qBndNV/Ngv4mpfxqOpqKbFFhkmNR9Jxfrt4Y08vB7qaaGFeu4INxJGA45ERkJJQSpWQwPMv6OR25KNRCJw8w804EhkpJQUpGJFo7BunS5UFhlNSgpS/nIXR+jzyR+NKhmIjCYlBSlviQSp+QuwZBKPRAiva1UWECmgQUcfmdkexQhEZBeJBG8tXQ6dnYQ8RbozyR+b46WOSmRMy2dI6u/N7CYz00zDUjyZaSs+8PhDhEjTTYguIjxMrNSRiYxp+SSFvwCeBm4zs0fN7DwzqylsWDJeJRKwYgX8sTmOdyYJkSZFiFZO5PRIKzMb1HQkUkiDJgV3f9/df+Tuc4ElwLHAC2b2XTObWfAIZdxIJGBZLMF7/7CCa35Uy/Z0hC7CJKnh3z+znBXxqLoTRAps0I5mMzsE+DNgr8z9o8BLwNeAS4BwIQOU8eOly5tYm1xMiDTJVA1LWUktHTwSinHGXCUEkWLIZ/TR74H/AO4F3gLeA94F/nfmXmREEgl45NoES9cvoZruzFxGnXwo3MF3WEYkAtfFShujyHiRT1KYDSwC5gN3A83u/l8FjUrGjZ4mo2XJ5YQzCcEBLMQXbo6xp65QFimqQZOCuz8NfN3M9gS+DKw2s9eAW939NwWOT8a4PzQnWJNcQIROQjgpQqQI89qlN1LXGEVD3kSKaygXr6WBfwXWAZ8CbjYz3P0jBYlMxry2pgQfe2A5ETqpygw73Tz5RNLfWk5do6oGIqWQT0fzW5kf3wf+J3N7F3gOeCffFzKzU4EbCDqmf+zu3+mnTAxYCVQDb7r7CfkeXypLW1OCGYuCGkI4M+zUamo44l+Wq61IpITyqSns4+4+eLGBmVkYuAk4CWgHnjCz+9z9+ZwyewM3A6e6+6tm9qGRvKaUt46WOIeRzNYQXj3kRD7cvFwJQaTE8rlOYUQJIWMu8KK7v+zuSYIO64V9ynwZ+Lm7v5p53T+NwutKmaqtj5Fk53UI71+6XAlBpAwUa0K8g4DXcrbbgU/0KXMoUG1mcYJrIm5w9+bihCfFVtcYpY1WOlri1NbH1IcgUiaGnBTM7CR3//VQn9bPY31rIFXAx4AFwEQgYWaPufvv+7x+I9AIMHXq1CGGIeWkrjEKSgYiZWU4azRfM4zntANTcrYnA1v7KfOrzLQabwLrgaP6Hsjdm9x9jrvP2W+//YYRihRVz2RGiUSpIxGRPBSr+egJYKaZTSe4OvqLBH0Iuf4VuNHMqoAIQfPS94sUnxRCZqZTkslg4eRWrYUgUu7ySgpmdgdBc48BU83s9p597n7hYM93924zWwI8SDAk9XZ3f87MLs7sv9XdXzCzXwHPEFwT8WN3f3bIv5GUXM9CaV9+Nc7BySSkUkFiiMeVFETKXL41hTtzfv4k8JOhvpC7rwHW9Hns1j7b1wHXDfXYUj5yKwcPhmM8FIpg6SRURQjHYqUOT0QGkVdScPeHe342s3dzt0VyxeMwuzPBcek469Mx5odaOZ44v/UYK4iieoJIeRtOn0Jy1KOQMePM2gTfTC8gQpKkRzgp3crVvoxwSq1HIpVgyKOP3P2YQgQiY0NdR5yJoSRVpJgQSrIgHCccDvqZ1XokUv6KNfpIxotYDKuJQDJJKBLhcytjTNT01yIVQ0lBRlc0Ggw9jcchFqMuqumvRSrJcK5o3hPY4e6pAsQjY0E0qmqBSIUatE/BzEJm9mUz+4WZ/Qn4HfC6mT1nZteZ2czChykiIsWQT0fzOmAGsAzY392nuPuHgOOAx4DvmNm5BYxRRESKJJ/moxPdvavvg+7+30AL0GJm1aMemZSdniuV1WksMnblkxR+bGZfy6yD0K/+koaMDT2JoLYWli7VNEYiY10+SeE1gmms6919S8+DZnYksDSfuY+kMuVOWWEG6XRw0zRGImPXoEnB3f/RzB4DHjKzbxKsn7yUzEI4hQ1PSikeDxJAKgWhEBxrCY63OL8Nx4jFlBFExqJ8h6SuB34F3A/8Cfi8u68vWFRSFmKxoKkomYRGa+KH6SUYKdxqCNMKmslIZMzJZ0jqTUAb8B5wGPAb4K/NbI8CxyYlFo3CypWw5GMJfpheTDjdRcjThLs6g2qEiIw5+QxJbQM+4u6Xu/tmd/8ykAAeM7NDCxuelFIiAU8vbuIbjzfg6W4gs4ZqOKyJjETGqHz6FG7t57HvmdlTBOsjHFKIwKT03rq2iZu6F2W3u4E0Vbx2yY3MUC+zyJiUT/OR9fe4u/8GmL+7MlLZoltbgGC5PYCXOYRPhdbzL3s3li4oESmofJqPfmNmf2VmU3MfNLMIwbrLPwHOL0h0UlIfvKgeyDQZAd+zS3myJqqWI5ExLJ/RR38AUsC9ZnYA8DYwgWCt5bXA99396UIFKCXU2BjUElpaeOmj9Uzbu5HWmK5PEBnLzN13X8DsWXefZWZPAnOB/YDt7v52EeLbrTlz5vjGjRtLHYaISEUxs03uPqe/ffk0Hz1oZgngz4EG4EBgxyjGJyWUSMCKFcG9iEg+o4/+xsw+DMSB6cBfAkeYWRJ41t2/UNgQpVByp7HQfEYiAnle0ezuL5vZie7++57HzGwSMKtgkUnBxeNw9I4Ex3uc9TtixONRJQWRcS7vlddyE0Jm+z2C9RSkQh32doJLPEY1XXR5Nb96O46mrhAZ3/LpU5Axamq8mRqShHFqSDI13lzqkESkxIaUFMzsU7n3Upl6OpdrJvR+/MADSxOPiJSPodYUvtvnXipMUxNcdlyCd/9hBTdtOJpUVQ1pDK+uYf/LGkodnoiUWN59Cn1oWosKlEjAk19vojW9mBBpkt01rP3MDzh9bofW2BQRYPhJQSrQH5oT/CC9hGq6M1m9kyP274Bly0ocmYiUC3U0jyMnECdMCiOYz8hCIQ5uiJU4KhEpJ0VLCmZ2qpltNrMXzezy3ZT7uJmlzOzsYsU2XhzcEIOaGtIWwsPVhG+5SU1GItLLUJuP3svcvzuUJ5lZGLgJOAloB54ws/vc/fl+yl0DPDjEuGQAiUTQbHQCcQ5uiBFe1xpctaY+BBHpx5CSgrsfn3s/BHOBF939ZQAzuxtYCDzfp9xfAS3Ax4d4fOlHIgH/dHwTK7uXECJF9201VD3cqj4EERlQsZqPDgJey9luzzyWZWYHAZ8FdlnpTYbnD80JVnYvppouqkhDVyd/bI6XOiwRKWP5rLx27ii8Tn9DWPvO2b0S+Dt3Tw0ST6OZbTSzjW+88cYohDb2JBLQ/PUE89cvJ5TTsZwmzMPEShuciJS1fJqPzjOzjwP/a7AP7N1oB6bkbE8GtvYpMwe4O7Oy577A6WbW7e6rcwu5exPQBMF6CsOMZ8za2WS0mBApwjjdGGnCLK26kfMa1I8gIgPLp/noVGA7wbKcHxrm6zxBsHTn9Mwynl8E7sst4O7T3X2au08D7gG+0TchyOCCJqPgWoQqnBTG64efxN0Xr+e89Y3qWxaR3cpnPQUHLjezs4D1ZnY98DTBWgrb8nkRd+82syUEo4rCwO3u/pyZXZzZr36EUXLSfzYTzlyc1tNktO745TTcomwgIoPLa/SRmZ0JfBVIArOBcwkW2nnL3Q/J5xjuvgZY0+exfpOBu38ln2NKH01NHHD/j3AcB7oJs7TqJjUZiUjeBk0KZvYy8ALwfXf/dZ99kwsVmAxRIkH6G4uxVE/HstE292uct1JNRiKSv3xqCqe7++/62+Hu7aMcjwzTH5vjHJRKU0XQbNRFFdUXNTBbCUFEhmDQjuaBEoKUl4eJkaSGbkJ0UcUSbuSBDmUEERkazZI6RsxsiHLqj1uZ1x0nToynaqKsi5U6KhGpNEoKFaytKUFHS5za+hjRxijXrI/S3Bzlo8D1DZraSESGTkmhQrU1JZixaAGHkSS5NkIbrUQbo0oEIjIiWk+hEiUS7HndciJ0UkWKapJ0tMRLHZWIjAGqKVSaRAIWLGD6jk4gnelYjlBbHyt1ZCIyBqimUGnicbwziXkatxCvHnIiL61qpa5R7UYiMnJKChWmrTbG9nSELsLs8Brev3S5EoKIjBo1H5W5RKL3QmkPdET5RaiV49JxHgnFOKMjSl2pgxSRMUNJoYwlEkEy6OqC6uqdyeHKmiiPJaNEInBdrLQxisjYouajMtbcDA3JJtb4KTQkm2huDmoLra1w5ZXBvYagishoUk2hjM17volzWATAKazlp88DBBPcKRmISCGoplDGztjRAuxcy7RnW0SkUJQUytgHL6oHdi5m3bMtIlIoaj4qZ42NQS2hpQXq66GxsdQRicgYp6RQ7hoblQxEpGjUfCQiIllKCuUikYAVK4J7EZESUfNRCfVcrfz5t5uY/r0lkE7hkRrC63QBgoiUhpJCiWQmO2V2Z4K/SS/G6MaA7s5O/tgc52AlBREpATUflUg8HiSEf0wvJ0QKIxh6mibMw8RKG5yIjFuqKZTImbUJvpleQIROwjjdGGnC/HXoRs5vUC1BREpDNYUSqfvltUxkO1WkSRGilZNYEF7P7Fsa1Z0gIiWjmkIpNDXhq1cDQZNRKBzmv762nGsbtMayiJSWkkIJvHZDC5Mh24/wxpSjabhF2UBESk/NRyXQ4r3nNLpr4kWlC0ZEJIdqCkXS1pSgoyVObX2MPZY20rgI6mmhhXo+vlTTWIhIeVBSKIK2pgQzFi3gMJIk10ZgVSusauT7LY2a505EykrRmo/M7FQz22xmL5rZ5f3sP8fMnsncNpjZUcWKrdA6WuJESFJFimqSdLTEaWyEBx9UQhCR8lKUpGBmYeAm4DTgcOBLZnZ4n2KvACe4+5HAlUBTMWIrhtr6GEkidBGmiwi19bFShyQi0q9iNR/NBV5095cBzOxuYCHwfE8Bd9+QU/4xYHKRYiu4usYobbRm+xTqGjXSSETKU7GSwkHAaznb7cAndlP+IuCXBY2oyOoao6BkICJlrlhJwfp5zPt5DDObT5AUPjnA/kagEWDq1KmjFd+o6pn9NBbTZKciUlmK1dHcDkzJ2Z4MbO1byMyOBH4MLHT3jv4O5O5N7j7H3efst99+BQl2JBIJWBZL8N4/rGBZLKHlEUSkohSrpvAEMNPMpgP/AXwR+HJuATObCvwcOM/df1+kuEbdH5oTrEkuIEKSZDLCPc2tRFVdEJEKUZSk4O7dZrYEeBAIA7e7+3NmdnFm/63AFUAtcLOZAXS7+5xixDeaTmDn8FMnyQnEASUFEakM5t5v035FmDNnjm/cuLHUYfTuQyBBav4CSCYhEtEqaiJSdsxs00BfunVF8wg1NcGSJZBKQU0NtLZGia5rVU+ziFQkJYURSCRg8WKY050gRpz1O2LE41Giy6JKBiJSkZQURiAehwu6m7iRJYRIkfQaXqptRX0IIlKplBRG4MzaBH/BYqrpxoCQdVLXEUdJQUQqlZLCCNR1xHFLY55ZQa0qHPQjiIhUKC2yMwxNTXDKKbD67Rg2oQZCIayqCm68UX0JIlLRVFMYoqYmeGJRE5fQQsvaerislc/sHddIIxEZE5QUhugDV/8dTVwLwCms5Yb7V8Hzy0oclYjI6FDz0VAkEpz96neBnTP81VtL6eIRERllSgpD8Pq1zeBpjJ1TvE75Zn0pQxIRGVVKCvlKJNjv/jsIESSENEbi+Mu0nqaIjClKCnloa0rwcsNyQqkujCAh3BZehH3nmlKHJiIyqtTRPIi2pgQzF80nQhLD6SZEOlxD9OYG6jTYSETGGNUUBtF1WzM1dBLK9CJsZA4nh1t5TxlBRMYgJYVBHHBg7+2nmM2jqSjxeEnCEREpKCWFQbx5WgOdREhhdBLhn62BSESzWYjI2KQ+hUE80BHlF6E4x6XjrLcYk06K0rpcFy+LyNikpDCIWAyurInyWDJKJIISgoiMaUoKg4hGoVULqYnIOKGkkIeoFlITkXFCHc0iIpKlpCAiIllKCiIikqWkICIiWeMyKSQSsGJFcC8iIjuNu9FHiQQsWACzOxNsD8WZdFOMukYNLRIRgXGYFOJxOG9HEz/0JYTSKXxJDdS1asypiAjjsPnozNoEP/TFVNNFFWmqUp1odjsRkcC4qynUdcRxS2MerKBm4bBmtxMRyRh3NQViMWxCDW4h0qEqXrrkRjUdiYhkFC0pmNmpZrbZzF40s8v72W9m9oPM/mfMbHZBAolGaVvZyrfDV3EC66n7YaNGIYmIZBSl+cjMwsBNwElAO/CEmd3n7s/nFDsNmJm5fQK4JXM/6h7oiHK1R0mlIZwMuhRUWRARKV5NYS7woru/7O5J4G5gYZ8yC4FmDzwG7G1mBxQimFgMIhEIh9GCOSIiOYrV0XwQ8FrOdju71gL6K3MQ8HpuITNrBBoBpk6dOqxgNB22iEj/ipUUrJ/HfBhlcPcmoAlgzpw5u+zPl6bDFhHZVbGaj9qBKTnbk4GtwygjIiIFVKyk8AQw08ymm1kE+CJwX58y9wENmVFIxwDvuPvrfQ8kIiKFU5TmI3fvNrMlwINAGLjd3Z8zs4sz+28F1gCnAy8C24ALihGbiIjsVLQrmt19DcEHf+5jt+b87MDiYsUjIiK7Gn9XNIuIyICUFEREJMuCVpvKZGZvAH/Ms/i+wJsFDGe4yjUuUGzDUa5xgWIbjnKNC0YW28Huvl9/Oyo6KQyFmW109zmljqOvco0LFNtwlGtcoNiGo1zjgsLFpuYjERHJUlIQEZGs8ZQUmkodwADKNS5QbMNRrnGBYhuOco0LChTbuOlTEBGRwY2nmoKIiAxCSUFERLIqPimMZJnPwZ5bhNjOycT0jJltMLOjcvZtMbM2M3vazDYWOa6Ymb2Tee2nzeyKfJ9bhNguzYnrWTNLmdk+mX2FfM9uN7M/mdmzA+wv5Xk2WGwlOc/yjK0k51oecZXqPJtiZuvM7AUze87MvtlPmcKea+5esTeCyfVeAj4MRIB/Bw7vU+Z04JcE6zUcA/xbvs8tQmzHAh/M/HxaT2yZ7S3AviV6z2LAA8N5bqFj61P+08BvCv2eZY59PDAbeHaA/SU5z/KMrejn2RBiK9W5ttu4SnieHQDMzvy8F/D7Yn+mVXpNYSTLfObz3ILG5u4b3P2tzOZjBGtIFNpIfu+Sv2d9fAm4axRff0Duvh74790UKdV5NmhsJTrPel57sPdtIAV934YYVzHPs9fd/cnMz+8CLxCsQJmroOdapSeFgZbwzKdMPs8tdGy5LiLI/j0cWGtmmyxYgrTYcUXN7N/N7JdmdsQQn1vo2DCzPYBTgZachwv1nuWjVOfZUBXrPBuKUpxreSnleWZm04CjgX/rs6ug51rRps4ukJEs85nX8p8jkPfxzWw+wT/rJ3MenufuW83sQ8Cvzex3mW83xYjrSYK5Ud4zs9OB1cDMPJ9b6Nh6fBr4rbvnftsr1HuWj1KdZ3kr8nmWr1Kda/kqyXlmZpMIEtFSd/+fvrv7ecqonWuVXlMYyTKfhV7+M6/jm9mRwI+Bhe7e0fO4u2/N3P8JuJegaliUuNz9f9z9vczPa4BqM9s3n+cWOrYcX6RPlb6A71k+SnWe5aUE51leSniu5avo55mZVRMkhJ+6+8/7KVLYc60QnSXFuhHUdF4GprOzY+WIPmXOoHenzOP5PrcIsU0lWGnu2D6P7wnslfPzBuDUIsa1PzsvbJwLvJp5/0r+nmXKfYCgPXjPYrxnOa8xjYE7TEtynuUZW9HPsyHEVpJzbbC4SnWeZX73ZmDlbsoU9Fyr6OYjH8EynwM9t8ixXQHUAjebGUC3B7Me/jlwb+axKuBn7v6rIsZ1NvB1M+sGtgNf9OCsK4f3DOCzwFp3fz/n6QV7zwDM7C6CkTL7mlk78G2gOieukpxnecZW9PNsCLGV5FzLIy4owXkGzAPOA9rM7OnMY39PkNiLcq5pmgsREcmq9D4FEREZRUoKIiKSpaQgIiJZSgoiIpKlpCAiIllKCiIikqWkICIiWUoKIqPIzL5uZjfnbF9lZv9UyphEhkIXr4mMosysmpuBOoKJ564kmF5ie0kDE8mTkoLIKDOzawnmxTkNOMndXypxSCJ5U1IQGWVm9hGCxVEWuvt9pY5HZCjUpyAy+q4A3qDy1yuRcUhJQWQUmdnfABOAzwO7LLouUu70TUZklJjZpwimMY66+7tm9mdm9lF3f7rEoYnkTTUFkVFgZlMJVjb7nAcLrgPcACwtWVAiw6COZhERyVJNQUREspQUREQkS0lBRESylBRERCRLSUFERLKUFEREJEtJQUREsv4/dyAIBHEtct8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def F(x, D): # basis\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "    D - number of parameters.\n",
    "  \"\"\"\n",
    "  return np.power(x[:,None], np.arange(D)[None,:])\n",
    "\n",
    "def f(x, theta):\n",
    "  # axis 0 = # sample\n",
    "  # axis 1 = monomial degree\n",
    "  xn = F(x, D=len(theta))\n",
    "  return np.dot(xn, theta)\n",
    "\n",
    "y_optimal = f(X, theta_optimal)\n",
    "\n",
    "plt.plot(X,Y, '.b', label='samples')\n",
    "plt.plot(X,y_optimal, '.r', label='optimal')\n",
    "plt.xlabel('$X$')\n",
    "plt.ylabel('$Y=f(X)+W$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**. Calculate mean square error for obtained solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error 0.001332663688026644\n"
     ]
    }
   ],
   "source": [
    "def R(theta, x, y):\n",
    "  rn = (y - f(x, theta))**2\n",
    "  return np.sum(rn)\n",
    "\n",
    "print('Mean square error', R(theta=theta_optimal, x=X, y=Y)/np.sqrt(S) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Maximum likelihood principle**. Another way to approach machine learning problem is a search of maximum of some likelihood function. Likelihood function function is a probability of specific outcome. \n",
    "Given probability density $f_W$ for random noise $W$, we can estimate likelihood of outcomes resulting in samples $(x_n,y_n)$.\n",
    "[Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) for the continuous destribution can be written as follows:\n",
    "$$\n",
    "L(\\theta|x,y) = \\prod_n f_W(y_n-f(x_n,\\theta)). \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log L(\\theta|x,y) = \\sum_n \\log f_W(y_n-f(x_n,\\theta)). \n",
    "$$\n",
    "\n",
    "Maximum of the likelihood function corresponds to the value of $\\theta$ such that outcomes $(x_n,y_n)$ are most probable,\n",
    "thus we obtain the maximum likelihood estimate for the parameters \n",
    "\n",
    "$$\n",
    "\\hat\\theta = \\mathrm{argmax}_\\theta \\log L(\\theta|x,y).\n",
    "$$\n",
    "\n",
    "**Problem 1.** Which distribution of $W$ does correspond to least squares method (minimum of mean square error coincides with maximum of likelihood function)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**.\n",
    "From the point of view of machine learning (ML), the function $R$ is a loss function measuring performance of the artificial neural network (ANN) $f$ with parameters $\\theta$. \n",
    "The network is used to predict the value of $Y$ given a value of $X$.  \n",
    "In the context of ML the optimization of $\\theta$ above is called training of the algorithm/network.\n",
    "\n",
    "The simplest method to train an algorithm is the [steepest gradient method](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "Given an initial vector of parameters $\\theta$, the parameters are updated on each step of optimization by the formula:\n",
    "\n",
    "$$\n",
    "\\theta\\mapsto \\theta-\\alpha\\frac{\\partial R}{\\partial\\theta}[\\theta],\n",
    "$$\n",
    "until the method converges, i.e. the stop conditions are satisfied, e.g. the loss $R$ is small enough,\n",
    "or its update $\\partial R/\\partial \\theta$ is small.\n",
    "The parameter $\\alpha$ is learning rate in ML or step size in the numerical methods, it is a metaparameter, i.e. it does not belong to the model, but may affect the result of optimization. \n",
    "In the simplest form of the gradient descend the learning rate is constant, however often sofisticated method of variation of $\\alpha$ are required \n",
    "to stabilze the method or improve convergence, e.g. [Adam method](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam).\n",
    "Higher order method, such as [LBFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm), can also be used to improve convergence, however the methods are used less often.\n",
    "\n",
    "The number of samples sometimes much larger than required to estimate the direction of optimization reliably,\n",
    "in the case the extra points only slow down the computation of the gradient.\n",
    "Therefore [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) is commonly used in ML instead of the gradient method.\n",
    "In the method only a subset (batch) of the samples is used to compute the gradient of loss,\n",
    "the batch is regenerated/resampled after few iterations of optimization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most well-known variant of the least square method is the linear one/linear regression.\n",
    "In the case prediction function $f$ depends on $\\theta$ linearly:\n",
    "\n",
    "$$\n",
    "f(x,\\theta)=\\sum_k f_k(x)\\theta_k,\n",
    "$$\n",
    "where $f_k$ is a set of predefined functions forming a basis (for instance, in **Example 2** we used the basis of polynomials).\n",
    "Then optimality conditions takes form of a linear system with respect to $\\theta$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial R}{\\partial\\theta_k} = 2\\sum_n\\bigg(y_n-\\sum_j f_j(x_n)\\theta_j\\bigg)f_k(x_n) = 0\\forall k.\n",
    "$$\n",
    "\n",
    "$$\n",
    "F^T y - F^T F \\theta = 0,\n",
    "$$\n",
    "where $F$ is a matrix with elements $F_{nk}=f_k(x_n)$\n",
    "\n",
    "$$\n",
    "F^T F \\theta=F^T y.\n",
    "$$\n",
    "The solution to the problem is given by the [Moore-Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of the matrix $F$:\n",
    "\n",
    "$$\n",
    "\\hat \\theta=(F^T F)^{-1}(F^T y).\n",
    "$$ \n",
    "Numerically the solution can be computed without computation the square of the matrix $F$ using e.g. [QR-decomposition](https://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares).\n",
    "\n",
    "Unfortunatelly, it is hard to solve least square problem explicitly for dependencies $f$ more complex than linear. \n",
    "Hopefully, ML methods can be applied to arbitrary sufficiently smooth functions $f$.\n",
    "\n",
    "**Example 4**. Implement function to solve least square problem and compere obtained parameter value with optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt: [ 0.          1.          0.         -0.16666667  0.          0.00833333]\n",
      "lsq: [ 0.00167969  1.01102619 -0.09384331 -0.00798737 -0.09767579  0.02790537]\n"
     ]
    }
   ],
   "source": [
    "def lsq(x, y, D):\n",
    "  f = F(x, D)\n",
    "  theta = np.linalg.solve( f.T@f, f.T@y )\n",
    "  return theta\n",
    "\n",
    "D = len(theta_optimal)\n",
    "theta_lsq = lsq(X, Y, D)\n",
    "print(f'opt: {theta_optimal}')\n",
    "print(f'lsq: {theta_lsq}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.** Split the data set $(x_n,y_n)$ to the training set (80%) and testing set (20%) and choose optimal $D$ avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above we used explicit results of differentiation but in practice gradient calculation performed by computer. For example, [``autograd``](https://github.com/HIPS/autograd) package is a tool to automatically differentiate native Python and Numpy code.\n",
    "\n",
    "**Example 5** Create data array, define function and calculate its gradient using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,t) = -113.69583801751507\n",
      "df/dx =  [  0.  15.  30.  45.  60.  75.  90. 105. 120. 135.]\n"
     ]
    }
   ],
   "source": [
    "# import autograd as ag\n",
    "# function that perform scalar product of vectors 'x' with 'n' multiplied by 't'\n",
    "def f(x,t):\n",
    "    n = np.arange(x.shape[0]) # create 'n' vector as array of numbers from 0 to length of 'x'\n",
    "    return np.sum(x*n)*t\n",
    "\n",
    "x = np.random.randn(10) # take input data as array of 10 random values\n",
    "t = 15 # parameter\n",
    "print('f(x,t) =',f(x,t) )\n",
    "# ag.grad(f,y) - provides a partial derivative of 'f' function with respect 'y' variable  \n",
    "df = ag.grad(f,0) #define gradient\n",
    "print('df/dx = ',df(x,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: autograd works with non integer arrays because you can not differentiate integer data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some assumptions in previous consideration.\n",
    "\n",
    "Firstly, in **least squares method** we assumed that we have a linear dependence from parameters but in the case of non-linear dependence we also need to use non-linear approximation.\n",
    "\n",
    "**Problem 3.** Using [``autograd``](https://github.com/HIPS/autograd) package, try another non-linear approximation of $y(x)$, \n",
    "e.g. by [rational function](https://en.wikipedia.org/wiki/Rational_function) (see also [Pad√© approximant](https://en.wikipedia.org/wiki/Pad%C3%A9_approximant)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we assumed that noise is a random value with normal probability distribution. \n",
    "Let p and q are probability distributions of random values 'x' and 'y'. The cross-entropy of this distribution can be defined as\n",
    "$H(p,q) = -\\sum_{x\\in X} p(x)\\, \\log q(x)$\n",
    "\n",
    "**Problem 4.** Try to use [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) as a loss function to evaluate distribution of the error term $W=Y-f(X)$. \n",
    "Compare the result with the least squares method. \n",
    "Train the method using noise with any not normal probability distribution.\n",
    "\n",
    "For example:\n",
    "\n",
    "a) [Student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) for noise $W$: $f_W (x) = \\dfrac{\\Gamma\\left(\\dfrac{\\nu+1}{2}\\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma\\left(\\dfrac{\\nu}{2}\\right)} \\left(1+\\dfrac{x^2}\\nu \\right)^{-(\\nu+1)/2}$\n",
    "\n",
    "b) Uniform distribution for noise $ f_W = \\dfrac{1}{b-a}$ for $a<x<b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick recap**\n",
    "1) Define model\n",
    "2) Separate data on training and testing parts\n",
    "3) Calculate gradient (for example, using autograd)\n",
    "4) Optimize \n",
    "5) Check overfitting (for example cross-validation)\n",
    "\n",
    "**Example 6** Find optimal value of parameter $\\theta$ where minimum of Loss function $R(x,y,\\theta)$ is achieved using autograd.\n",
    "\n",
    "Here we will use loss function from **Example 3** defined as a mean square error.\n",
    "We start from definition of a gradient and then introduce optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.67588056  1.20962699  2.15269319  3.92472591  7.25449487 13.52678604]\n",
      "[ 3.92255672e-14  4.55885329e-15  6.46982468e-14 -3.64430708e-14\n",
      "  2.28074504e-13 -1.92450222e-13]\n"
     ]
    }
   ],
   "source": [
    "dR = ag.grad(fun=R, argnum=0)\n",
    "# print( R(theta_optimal, X, Y) )\n",
    "print( dR(theta_optimal, X, Y) )\n",
    "print( dR(theta_lsq, X, Y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = theta_optimal\n",
    "\n",
    "def optimize(x,y, theta, alpha, tol=1e-6, maxiter=10000):\n",
    "  for it in range(maxiter):\n",
    "    loss = R(theta=theta, x=x, y=y)\n",
    "    dloss = dR(theta, x, y)\n",
    "    norm_dloss = np.linalg.norm(dloss.flatten())\n",
    "    if it%100 == 0:\n",
    "      print(f\"{it}: {loss} {norm_dloss}\")\n",
    "    if norm_dloss<tol: return theta\n",
    "    theta = theta - alpha*dloss\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.012815808482605853 16.048651338161353\n",
      "100: 0.008705586247978032 0.04173765884251222\n",
      "200: 0.008700248222268397 0.02767522505797952\n",
      "300: 0.008696944115468565 0.024031597281412544\n",
      "400: 0.008694341847016044 0.021709931266575246\n",
      "500: 0.008692164419809394 0.020104653421873956\n",
      "600: 0.00869026063801345 0.018973482501753527\n",
      "700: 0.008688541003655192 0.018150553073702357\n",
      "800: 0.008686951988648519 0.017524316067604895\n",
      "900: 0.008685461220300165 0.017022501846967516\n",
      "1000: 0.008684048787775494 0.016599541531913845\n",
      "1100: 0.008682702135758643 0.01622720932344914\n",
      "1200: 0.008681413064413494 0.01588820984358086\n",
      "1300: 0.00868017596622917 0.01557200480472436\n",
      "1400: 0.008678986789231308 0.015272182797280263\n",
      "1500: 0.008677842426776433 0.014984834166415118\n",
      "1600: 0.008676740357896739 0.014707559089138714\n",
      "1700: 0.008675678434824148 0.01443886666345943\n",
      "1800: 0.008674654756992485 0.01417781254667991\n",
      "1900: 0.008673667595871645 0.013923781058188697\n",
      "2000: 0.008672715349699857 0.013676354375171042\n",
      "2100: 0.008671796515819712 0.01343523408810752\n",
      "2200: 0.0086709096733967 0.01320019418050463\n",
      "2300: 0.008670053472277908 0.01297105284791329\n",
      "2400: 0.00866922662549818 0.01274765560369534\n",
      "2500: 0.008668427903967784 0.012529865143851421\n",
      "2600: 0.008667656132479865 0.012317555258490393\n",
      "2700: 0.00866691018652893 0.012110607165615336\n",
      "2800: 0.008666188989641707 0.011908907294980195\n",
      "2900: 0.008665491511041483 0.011712345940037925\n",
      "3000: 0.008664816763541265 0.011520816429830617\n",
      "3100: 0.008664163801602415 0.01133421461248284\n",
      "3200: 0.008663531719519 0.011152438525710785\n",
      "3300: 0.008662919649705749 0.010975388179842754\n",
      "3400: 0.008662326761072947 0.010802965408798331\n",
      "3500: 0.008661752257478937 0.010635073762399978\n",
      "3600: 0.008661195376253022 0.010471618424119828\n",
      "3700: 0.008660655386783666 0.010312506144782142\n",
      "3800: 0.008660131589167773 0.010157645186521144\n",
      "3900: 0.008659623312917959 0.01000694527368292\n",
      "4000: 0.008659129915724801 0.009860317548616031\n",
      "4100: 0.008658650782271437 0.009717674531188948\n",
      "4200: 0.008658185323098314 0.009578930081321902\n",
      "4300: 0.008657732973515702 0.009443999364137211\n",
      "4400: 0.008657293192562152 0.009312798817451473\n",
      "4500: 0.00865686546200666 0.009185246121485587\n",
      "4600: 0.008656449285392854 0.009061260170707068\n",
      "4700: 0.008656044187123443 0.00894076104771987\n",
      "4800: 0.008655649711583125 0.008823669999166775\n",
      "4900: 0.008655265422298282 0.008709909413627172\n",
      "5000: 0.008654890901132388 0.008599402801428409\n",
      "5100: 0.00865452574751456 0.00849207477636401\n",
      "5200: 0.008654169577701418 0.008387851039252256\n",
      "5300: 0.008653822024069158 0.008286658363280236\n",
      "5400: 0.008653482734436023 0.008188424581093213\n",
      "5500: 0.008653151371412867 0.008093078573519338\n",
      "5600: 0.008652827611781225 0.008000550259895493\n",
      "5700: 0.008652511145897481 0.007910770589897016\n",
      "5800: 0.008652201677122067 0.007823671536770813\n",
      "5900: 0.008651898921272662 0.00773918609189197\n",
      "6000: 0.008651602606100452 0.007657248260540597\n",
      "6100: 0.008651312470788314 0.007577793058791446\n",
      "6200: 0.008651028265470229 0.00750075651141361\n",
      "6300: 0.008650749750770802 0.007426075650659767\n",
      "6400: 0.008650476697364051 0.007353688515836228\n",
      "6500: 0.008650208885551035 0.007283534153558576\n",
      "6600: 0.008649946104854948 0.0072155526185336575\n",
      "6700: 0.008649688153633567 0.007149684974800475\n",
      "6800: 0.008649434838707355 0.007085873297290123\n",
      "6900: 0.008649185975004199 0.007024060673600882\n",
      "7000: 0.00864894138521812 0.006964191205883285\n",
      "7100: 0.008648700899482912 0.0069062100127312105\n",
      "7200: 0.00864846435505936 0.006850063230974013\n",
      "7300: 0.008648231596035337 0.006795698017283675\n",
      "7400: 0.00864800247303915 0.006743062549507953\n",
      "7500: 0.008647776842964234 0.006692106027637398\n",
      "7600: 0.008647554568705848 0.006642778674349927\n",
      "7700: 0.008647335518908743 0.006595031735045621\n",
      "7800: 0.008647119567725445 0.006548817477323821\n",
      "7900: 0.008646906594584553 0.006504089189839398\n",
      "8000: 0.008646696483969043 0.006460801180503198\n",
      "8100: 0.008646489125203782 0.006418908773977704\n",
      "8200: 0.008646284412251846 0.0063783683084374905\n",
      "8300: 0.008646082243519556 0.006339137131580437\n",
      "8400: 0.008645882521669736 0.006301173595862176\n",
      "8500: 0.008645685153442592 0.006264437052946251\n",
      "8600: 0.008645490049484561 0.006228887847367468\n",
      "8700: 0.008645297124183765 0.006194487309412025\n",
      "8800: 0.008645106295512933 0.006161197747219542\n",
      "8900: 0.008644917484878636 0.00612898243811966\n",
      "9000: 0.008644730616976841 0.006097805619228536\n",
      "9100: 0.008644545619654685 0.00606763247731723\n",
      "9200: 0.008644362423778106 0.00603842913798587\n",
      "9300: 0.008644180963104669 0.006010162654174352\n",
      "9400: 0.008644001174162294 0.005982800994033854\n",
      "9500: 0.008643822996132502 0.005956313028208744\n",
      "9600: 0.008643646370739146 0.005930668516550054\n",
      "9700: 0.00864347124214127 0.005905838094315242\n",
      "9800: 0.00864329755683101 0.005881793257887731\n",
      "9900: 0.008643125263535183 0.005858506350046879\n",
      "[ 1.09486814e-03  9.96004641e-01  8.21202983e-04 -1.64523275e-01\n",
      "  3.47047365e-04  7.08199271e-03]\n"
     ]
    }
   ],
   "source": [
    "theta0 = optimize(x=X, y=Y, theta=theta0, alpha=5e-5)\n",
    "print(theta0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
